# AISOC 2025

# SoundSentry – Language-Based Audio Retrieval: Real-Time, Privacy-First Multimedia Search App

## 👥 Team Details

* **Team Name:** SoundSentry
* **Member 1:** Monaal ([[monaalmamen@gmail.com](mailto:monaalmamen@gmail.com)](mailto:monaalmamen@gmail.com))
* **Member 2:** Sarthak Verma ([[iamsarthakverma22@gmail.com](mailto:iamsarthakverma22@gmail.com)](mailto:iamsarthakverma22@gmail.com))
* **Member 3:** Tarun ([[tarunmehrda@gmail.com](mailto:tarunmehrda@gmail.com)](mailto:tarunmehrda@gmail.com))
* **Member 4:** Alok Kumar ([[alokyadav1023@gmail.com](mailto:alokyadav1023@gmail.com)](mailto:alokyadav1023@gmail.com))
* **Member 5:** Abhinay Tiwari ([[abhinaytiwari542@gmail.com](mailto:abhinaytiwari542@gmail.com)](mailto:abhinaytiwari542@gmail.com))

## 🔍 Problem Statement

### ✨ Critical Market Gaps

Most audio search systems rely on fixed labels or manual tagging and lack the ability to retrieve sound clips based on flexible natural language input. Popular tools stream data to the cloud, raising privacy concerns, and are often too slow or heavy for real-time, edge-device use. There is a growing need for a fast, lightweight, privacy-preserving audio retrieval system that allows users to search using simple descriptive queries like "dog barking" or "rain hitting a window."

### 🤝 Human-Centric Use Cases

* **Accessible Sound Browsing:** Users with hearing or visual impairments can use plain text to explore sound databases.
* **Media & Film Production:** Editors and sound designers can search large audio libraries with natural language descriptions.
* **Security & Smart Monitoring:** Users can type queries like "glass breaking" to find relevant audio events in surveillance recordings.

## 🚀 Motivation

Existing audio retrieval systems lack the flexibility and accessibility of natural language search. Our mission is to empower users to explore and retrieve sound data using text prompts, all while preserving privacy. We focus on creating a clean, lightweight system that can use pre-captioned datasets like Clotho to build a working retrieval pipeline based on simple vector comparisons and feature extraction.

## 🌟 Expected Outcome

* 🎧 A fully functional app that takes natural language input and retrieves matching audio clips
* 📡 On-device inference using lightweight logic
* 📊 Embedding visualizations and similarity rankings
* 📲 Mobile/web app with clean UI and real-time feedback
* 📦 Open-source codebase and accessible public demo

## 📅 Timeline & Milestones

* **Week 1–2:** Deep learning basics, dataset understanding 
* **Week 3–4:** Build text-audio embedding and retrieval engine
* **Week 4–5:** Design search interface and result visualizer
* **Week 6:** Optimize inference, privacy filters, mobile packaging
* **Week 7–8:** Testing, documentation, deployment

## 📦 Final Deliverables

* Public hosted demo
* GitHub repository with full code
* Final presentation + Google Drive archive
* Use-case videos and documentation
* Dataset usage logs and final issue tracker

## 📌 Appendix

* **GitHub:** [https://github.com/Monaal5/Sound\_Sentry](https://github.com/Monaal5/Sound_Sentry)


## 🔗 References

1. Drossos, K., et al. (2020). Clotho: A Dataset for Audio Captioning – arXiv
2. Davis, S. B. (1980). MFCC Methodology – IEEE
3. Valin, J. M. (2018). RNNoise – arXiv
4. TensorFlow Lite Optimization Toolkit
5. DCASE 2024 Task 8 Overview and Evaluation Criteria

##  Get Started

```bash
# Clone the repository
git clone https://github.com/Monaal5/Sound_Sentry.git
cd Sound_Sentry

# Install dependencies
flutter pub get

# Run the app (emulator or device must be connected)
flutter run



---


# Sound Sentry Project

## 🔗 External Assets

Due to GitHub file size limits, large model and dependency files are stored externally.
I have added the complete ml code with all the requirements in the google drive.and demo videos and screen shots too


📁 [Download from Google Drive][https://drive.google.com/drive/folders/1IN8PFTFOsTMQKVK46Kjcja5WOpjv6KxB?usp=drive_link](https://drive.google.com/drive/folders/1IN8PFTFOsTMQKVK46Kjcja5WOpjv6KxB?usp=drive_link)



📂 Place them in:
- `src/ml_project/models/` — for model files
- `src/ml_project/venv/` — if using local venv
